{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working status에 따른 분석 의미 확인\n",
    "## Multinomial classification을 활용한 working status에 따른 featurer값 의미 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 필요 module load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 64.0 KiB for an array with shape (8192,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5152827648f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## data load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma_train_origin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/BEE K/Desktop/dataset/A_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0ma_test_origin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/BEE K/Desktop/dataset/A_test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma_train_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma_train_origin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma_test_set\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0ma_test_origin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fHack\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fHack\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fHack\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fHack\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2057\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2058\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2059\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2060\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2061\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 64.0 KiB for an array with shape (8192,) and data type int64"
     ]
    }
   ],
   "source": [
    "## data load\n",
    "a_train_origin = pd.read_csv(\"C:/Users/BEE K/Desktop/dataset/A_train.csv\")\n",
    "a_test_origin = pd.read_csv(\"C:/Users/BEE K/Desktop/dataset/A_test.csv\")\n",
    "a_train_set = a_train_origin.copy()\n",
    "a_test_set =  a_test_origin.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 42개 null값 처리 후 필요 set 추출\n",
    "a_train_set = a_train_set.fillna(0)\n",
    "\n",
    "a_train_1 = a_train_set.loc[a_train_set[\"op_end\"]>0,:]\n",
    "a_test_1 = a_test_set.loc[a_train_set[\"op_end\"]>0,:]\n",
    "\n",
    "y_train_data = a_train_1[\"op_result\"]\n",
    "x_train_data = a_train_1.iloc[:,24:47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data set 전처리 (공장 가동중에 대한 데이터 추출)\n",
    "sp_df = a_train_set[((a_train_set[\"op_start\"]==1) | (a_train_set[\"op_end\"]==1)) ==True]\n",
    "sp_df\n",
    "sp_x_df = sp_df.iloc[:,24:47]\n",
    "sp_y_df = sp_df.loc[:,[\"op_start\",\"op_end\",\"op_result\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict용 test data set 전처리\n",
    "pp_df = a_test_set[((a_test_set[\"op_start\"]==1) | (a_test_set[\"op_end\"]==1)) ==True]\n",
    "pp_x_df=pp_df.loc[:,[\"seq\",\"d15\",\"d16\",\"d17\",\"d18\",\"d19\",\"d20\",\"d21\",\"d22\",\"d23\",\"d24\",\"d25\",\"d26\",\"d27\",\"d28\",\"d29\",\"d30\",\"d31\",\"d32\",\"d33\",\"d34\",\"d35\",\"d36\",\"d37\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data 정규화 및 train set / test set 분할\n",
    "\n",
    "sp_x_df.shape\n",
    "train_num = int(sp_x_df.shape[0] * 0.7)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(sp_x_df[:train_num].values)\n",
    "x_test = scaler.fit_transform(sp_x_df[train_num:].values)\n",
    "y_train = sp_y_df[\"op_result\"][:train_num].values.reshape([-1,1])\n",
    "y_test = sp_y_df[\"op_result\"][train_num:].values.reshape([-1,1])\n",
    "y_train.shape\n",
    "x_pre = scaler.fit_transform(pp_x_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가동 상태 분류\n",
    "a_train_set[\"new\"] = 0\n",
    "a_train_set.loc[a_train_set[\"op_start\"]==1,\"new\"] = 1\n",
    "a_train_set.loc[a_train_set[\"op_end\"]==1,\"new\"] = 2\n",
    "a_train_set.loc[a_train_set[\"op_result\"]==1,\"new\"] = 3\n",
    "\n",
    "# one-hot encoding\n",
    "new_y_val = pd.get_dummies(a_train_set[\"new\"])\n",
    "\n",
    "x_train_data = a_train_set.iloc[:,24:47]\n",
    "y_train_data = new_y_val\n",
    "\n",
    "# train set / test set 분리\n",
    "train_num = int(x_train_data.shape[0] * 0.7)\n",
    "\n",
    "# train data 정규화\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train_data[:train_num].values)\n",
    "x_test = scaler.fit_transform(x_train_data[train_num:].values)\n",
    "y_train = y_train_data[:train_num].values\n",
    "y_test = y_train_data[train_num:].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 정의\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 1. placeholder\n",
    "X = tf.placeholder(shape=[None, 23], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None, 4], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias 1\n",
    "W1 = tf.Variable(tf.random_normal([23,100]),name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([100]),name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "# Weight & bias 2\n",
    "W2 = tf.Variable(tf.random_normal([100,256]),name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]),name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "# Weight & bias 3\n",
    "W3 = tf.Variable(tf.random_normal([256,4]),name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([4]),name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer2, W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,\n",
    "                                                                   labels = Y))\n",
    "# train node\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_epoch = 50\n",
    "batch_size = 100\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(x_train.shape[0]/batch_size)\n",
    "    cost_val = 0\n",
    "    for i in range(num_of_iter):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        cut_train_x = x_train[start:end]\n",
    "        cut_train_y = y_train[start:end]\n",
    "        _, cost_val = sess.run([train, cost], \n",
    "                               feed_dict={ X : cut_train_x,\n",
    "                                            Y : cut_train_y})\n",
    "        \n",
    "    if step % 5 == 0:\n",
    "        print(\"Cost값 : {}\".format(cost_val))\n",
    "      \n",
    "#정확도 측정\n",
    "predict = tf.argmax(H, 1)\n",
    "correct = tf.equal(predict, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))\n",
    "print(\"정확도는 : {}\".format(sess.run(accuracy, feed_dict={X : x_test,\n",
    "                                                          Y : y_test})))\n",
    "\n",
    "# Cost값 : 0.6810373663902283\n",
    "# Cost값 : 0.31244540214538574\n",
    "# Cost값 : 0.3094421923160553\n",
    "# Cost값 : 0.3077668845653534\n",
    "# Cost값 : 0.3077707588672638\n",
    "# Cost값 : 0.30777063965797424\n",
    "# Cost값 : 0.3077707290649414\n",
    "# Cost값 : 0.3077709674835205\n",
    "# Cost값 : 0.30777060985565186\n",
    "# Cost값 : 0.30777081847190857\n",
    "# 정확도는 : 0.5890411734580994\n",
    "# Wall time: 6min 53s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 양품 / 불량품 분리하여 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 양품 / 불량품에 대한 그룹핑 (가동 중 result값을 불량품일때 0으로 처리 / 양품일때 1로 처리)\n",
    "a_train_set.iloc[:,24:47].sum()\n",
    "n = 0\n",
    "for i in range(int(len(sp_y_df))):\n",
    "    \n",
    "    if (sp_y_df.op_end[i:i+1].item() == 1) and (sp_y_df.op_result[i:i+1].item() == 0):\n",
    "        sp_y_df.op_result[n:i] = 0    \n",
    "        n = i\n",
    "    elif (sp_y_df.op_end[i:i+1].item() == 1) and (sp_y_df.op_result[i:i+1].item() == 1):\n",
    "        sp_y_df.op_result[n:i] = 1\n",
    "        n = i\n",
    "    else:\n",
    "        pass\n",
    "print(sp_y_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data 정규화 및 train set / test set 분할\n",
    "\n",
    "sp_x_df.shape\n",
    "train_num = int(sp_x_df.shape[0] * 0.7)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(sp_x_df[:train_num].values)\n",
    "x_test = scaler.fit_transform(sp_x_df[train_num:].values)\n",
    "y_train = sp_y_df[\"op_result\"][:train_num].values.reshape([-1,1])\n",
    "y_test = sp_y_df[\"op_result\"][train_num:].values.reshape([-1,1])\n",
    "y_train.shape\n",
    "x_pre = scaler.fit_transform(pp_x_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholer / reset\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(shape=[None,23], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "drop_rate = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "W1 = tf.get_variable(\"weight1\", shape=[23,100],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([100]), name=\"bias1\")\n",
    "L1 = tf.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.get_variable(\"weight2\", shape=[100,100],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([100]), name=\"bias2\")\n",
    "L2 = tf.sigmoid(tf.matmul(L1, W2)+b2)\n",
    "\n",
    "W3 = tf.get_variable(\"weight3\", shape=[100,1],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "# L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "logit = tf.matmul(L2, W3)+b3\n",
    "\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logit,\n",
    "                                                             labels = Y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(3000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={ X : x_train,\n",
    "                                                        Y : y_train})\n",
    "    if step % 300 == 0:\n",
    "        print(f\"Cost 값은 : {cost_val}\")\n",
    "        \n",
    "        \n",
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "correct = tf.equal(predict, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"정확도 : {}\".format(sess.run(accuracy, feed_dict={X: x_test,\n",
    "                                                        Y : y_test})))\n",
    "\n",
    "# Cost 값은 : 1.072007417678833\n",
    "# Cost 값은 : 0.020495368167757988\n",
    "# Cost 값은 : 0.02038237266242504\n",
    "# Cost 값은 : 0.020319107919931412\n",
    "# Cost 값은 : 0.020232681185007095\n",
    "# Cost 값은 : 0.020112425088882446\n",
    "# Cost 값은 : 0.019953615963459015\n",
    "# Cost 값은 : 0.019785946235060692\n",
    "# Cost 값은 : 0.0196489579975605\n",
    "# Cost 값은 : 0.01954224891960621\n",
    "# 정확도 : 0.9903903603553772"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = tf.cast(H > 0.5, dtype=tf.float32)\n",
    "\n",
    "print(f\"정확도 : {sess.run(predict,feed_dict={X : pp_x_df.iloc[:,1:]})}\")\n",
    "result = pd.DataFrame(sess.run(predict,feed_dict={X : pp_x_df.iloc[:,1:]}))\n",
    "result.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
